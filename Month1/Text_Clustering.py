import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer

"""The text must be parsed to remove words, called tokenization.
Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, 
called feature extraction (or vectorization)."""
#Reading the Data Set
data=pd.read_csv("C:\\Users\\kumar\\Internship\\Unfound.csv")
data=data.iloc[:,0:1]
#Finding and removing the Duplicate text for faster execution and also to achieve good results
data[data['SENTENCES'].duplicated(keep=False)].sort_values('SENTENCES').head(8)
data = data.drop_duplicates('SENTENCES')
#removing stop words as they don't have effect on clustering
punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)
desc = data['SENTENCES'].values
#The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents
vectorizer = TfidfVectorizer(stop_words = stop_words)
X = vectorizer.fit_transform(desc)
word_features = vectorizer.get_feature_names()
print(len(word_features))
print(word_features[5000:5100])
#Stemming is the process of reducing a word into its stem, i.e. its root form. 
#The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word.
stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'[a-zA-Z\']+')
#Tokenization is breaking the sentence into words and punctuation,
def tokenize(text):
    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]
#Vectorization with stop words(words irrelevant to the model), stemming and tokenizing
vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)
X2 = vectorizer2.fit_transform(desc)
word_features2 = vectorizer2.get_feature_names()
print(len(word_features2))
print(word_features2[:50]) 
vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)
X3 = vectorizer3.fit_transform(desc)
words = vectorizer3.get_feature_names()
#Considering X3 as it has features included
"""Clustering"""
# As we don't know the exact number of clusters to cluster the text WE USE ELBOW CURVE METHOD Which gives the exact number of clusters the data to be clustered
#Basically, number of clusters = the x-axis value of the point that is the corner of the "elbow"(the plot looks often looks like an elbow)
# Here i Used Knee Locator for elbow point
k_rng=range(1,13)
sse=[] # SUM OF SQUARED ERRORS
for k in k_rng:
    kmeans=KMeans(n_clusters=k,init='k-means++')#used kmeans++ algorithm as it chooses optimal number of clusters 
    
    kmeans.fit(X3)#kmeasn++ is efficient than kmeans algorithm
    sse.append(kmeans.inertia_)
plt.xlabel('K')
plt.ylabel('SSE')
plt.plot(k_rng,sse)
from kneed import KneeLocator
kn = KneeLocator(k_rng, sse, curve='convex', direction='decreasing')
print(kn.knee)
plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')
#
kmeans = KMeans(n_clusters = 10, n_init = 20, n_jobs = 1)
kmeans.fit(X3)
# Finally, we look at 8 the clusters generated by k-means.
common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]
for num, centroid in enumerate(common_words):
    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))
    #assigning eah cluster labble to text 
data['cluster']=kmeans.labels_
#copying data with its labels to a csv file
data.to_csv(r'C:\Users\kumar\Internship\Intern_Unfound\FinalResult.csv')
#each cluster label is stored in a seperate data frame.
data1=data[data.cluster==0]
data2=data[data.cluster==1]
data3=data[data.cluster==2]
data4=data[data.cluster==3]
data5=data[data.cluster==4]
data6=data[data.cluster==5]
data7=data[data.cluster==6]
data8=data[data.cluster==7]
data9=data[data.cluster==8]
data10=data[data.cluster==9]




